{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\serge\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf \n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec: skip gram & cbow\n",
    "\n",
    "Models __CBOW (Continuous Bag of Words)__ and __Skip gram__ were invented in the now distant 2013,\n",
    "*article*:\n",
    "[*Tomas Mikolov et al.*](https://arxiv.org/pdf/1301.3781v3.pdf)\n",
    "\n",
    "* __CBOW__ model predict missing word (focus word) using context (surrounding words).\n",
    "* __skip gram__ model is reverse to _CBOW_. It predicts context based on the word in focus.\n",
    "\n",
    "* **Context** is a fixed number of words to the left and right of the word in focus (see picture below). The length of the context is defined by the \"window\" parameter.\n",
    "\n",
    "![context](pics/context.png)\n",
    "\n",
    "Two models comparision\n",
    "\n",
    "![architecture](pics/architecture.png)\n",
    "\n",
    "\n",
    "### Skip_gram\n",
    "\n",
    "Consider a corpus with a sequence of words $ w_1, w_2, .., w_T $.\n",
    "\n",
    "Objective function (we would like to maximize it) for _skip gram_ is defined as follow:\n",
    "\n",
    "\n",
    "$$ AverageLogProbability = \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leqslant j\\leqslant c, j \\neq 0} log\\ p (w_{t+j} | w_t) $$\n",
    "\n",
    "* where $ c $ is a context length.\n",
    "* $w_t$ -- focus word\n",
    "\n",
    "The basic formulation for probability $ p (w_{t+j} | w_t) $ is calculated using __Softmax__ -\n",
    "\n",
    "$$ p (w_h | w_i) = \\frac{exp(s(v_i, v_h))}{ \\sum^{W}_{w=1}  exp(s(v_{w}, v_{i} )) } $$\n",
    "\n",
    "where\n",
    "* $w_i$ -- input focus word\n",
    "* $w_h$ -- hypothetically context word for a given focus word $w_i$\n",
    "* $v_i$ and $v_h$ input-word and hypothesis-word vector representations (for $w_i$, $w_h$)\n",
    "* $s(v_i, v_h) = v^{T} _{h} \\cdot v_{i}$\n",
    "* $W$ is the number of words in vocabulary\n",
    "\n",
    "___\n",
    "\n",
    "### CBOW\n",
    "\n",
    "Predict word using context.\n",
    "\n",
    "$$ E = -log\\ p(w_h\\ |\\ w_{1},\\ w_{2},\\ \\dots,\\ w_{c}) $$\n",
    "\n",
    "\n",
    "The **probability** is the same as in the *skip gram* model, but now $v_i$ is a sum of context-word vectors.\n",
    "\n",
    "$$ p(w_h\\ |\\ w_{1},\\ w_{2},\\ \\dots,\\ w_{c})  = \\frac{exp(s(v_i, v_h))}{\\sum^{W}_{w=1}  exp(s(v_{w}, v_{i}))} $$\n",
    "\n",
    "\n",
    "* $\\ w_{1},\\ w_{2},\\ \\dots,\\ w_{c}$ -- input context words\n",
    "* $w_h$ -- hypothetically focus word for a given context words\n",
    "* $ v_i = \\sum^{c}_{k=1} w_{k}$\n",
    "* $ v_h$ = vector of hypothesis word\n",
    "* $s(v_i, v_h) = v^{T} _{h} \\cdot v_{i}$\n",
    "* $W$ is the number of words in vocabulary\n",
    "\n",
    "___\n",
    "\n",
    "Lets implement __`Skip-gram`__ using tensorflow framework.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 2: preparing the data, building a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you prepare the data for the word2ve neural network by tokenizing it, building a dictionary and encoding it with corresponding identifiers. \n",
    "\n",
    "You may want to use the functions you have written during Seminar 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use this to prepare the 10 times larger file\n",
    "#!perl wikifil.pl enwik9>text9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the texts from csv file and convert them into a single list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = pd.read_csv(\"./train.csv\")['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts_joint = \" \".join(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text8') as infile:\n",
    "    all_texts = infile.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "# use any tokenizer you deem necessary\n",
    "tokens = nltk.regexp_tokenize(all_texts_joint, \"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(tokens, list)\n",
    "assert isinstance(tokens[0], str)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['USER_0',\n",
       " 'Unfortunately',\n",
       " 'it',\n",
       " 's',\n",
       " 'the',\n",
       " 'world',\n",
       " 'They',\n",
       " 'actually',\n",
       " 'had',\n",
       " 'a',\n",
       " 'news',\n",
       " 'segment',\n",
       " 'about',\n",
       " 'her',\n",
       " 'on',\n",
       " 'TV',\n",
       " 'in',\n",
       " 'Australia',\n",
       " 'last',\n",
       " 'night',\n",
       " 'why',\n",
       " 'my',\n",
       " 'tinyurl',\n",
       " 'is',\n",
       " 'bigger',\n",
       " 'than',\n",
       " 'my',\n",
       " 'real',\n",
       " 'url',\n",
       " 'URL_0',\n",
       " 'USER_0',\n",
       " 'you',\n",
       " 'think',\n",
       " 'he',\n",
       " 'called',\n",
       " 'JB',\n",
       " 'stupid',\n",
       " 'and',\n",
       " 'i',\n",
       " 've',\n",
       " 'gone',\n",
       " 'and',\n",
       " 'dropped',\n",
       " 'the',\n",
       " 'bloody',\n",
       " 'thing',\n",
       " 'in',\n",
       " 'my',\n",
       " 'coffee',\n",
       " 'USER_0',\n",
       " 'ahhh',\n",
       " 'i',\n",
       " 'was',\n",
       " 'just',\n",
       " 'there',\n",
       " 'sunday',\n",
       " 'night',\n",
       " 'USER_0',\n",
       " 'I',\n",
       " 'm',\n",
       " 'sure',\n",
       " 'Emily',\n",
       " 'will',\n",
       " 'want',\n",
       " 'to',\n",
       " 'tell',\n",
       " 'you',\n",
       " 'We',\n",
       " 'have',\n",
       " 'to',\n",
       " 'tell',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Fam',\n",
       " 'first',\n",
       " 'though',\n",
       " 'USER_0',\n",
       " 'Yes',\n",
       " 'Maddy',\n",
       " 'we',\n",
       " 'are',\n",
       " 'family',\n",
       " 'We',\n",
       " 're',\n",
       " 'both',\n",
       " 'cute',\n",
       " 'and',\n",
       " 'adorable',\n",
       " 'Fucking',\n",
       " 'way',\n",
       " 'bummed',\n",
       " 'i',\n",
       " 'didnt',\n",
       " 'get',\n",
       " 'to',\n",
       " 'go',\n",
       " 'to',\n",
       " 'EDC',\n",
       " 'Denver',\n",
       " 'this',\n",
       " 'past',\n",
       " 'weekend',\n",
       " 'cant',\n",
       " 'win',\n",
       " 'em',\n",
       " 'all',\n",
       " 'USER_0',\n",
       " 'lol',\n",
       " 'Love',\n",
       " 'your',\n",
       " 'BIO',\n",
       " 'crazy',\n",
       " 'twitter',\n",
       " 'lady',\n",
       " 'lol',\n",
       " 'Just',\n",
       " 'started',\n",
       " 'following',\n",
       " 'Looking',\n",
       " 'forward',\n",
       " 'to',\n",
       " 'getting',\n",
       " 'to',\n",
       " 'know',\n",
       " 'ya',\n",
       " 'I',\n",
       " 'don',\n",
       " 't',\n",
       " 'like',\n",
       " 'working',\n",
       " 'mornings',\n",
       " 'USER_0',\n",
       " 'To',\n",
       " 'be',\n",
       " 'honest',\n",
       " 'I',\n",
       " 'have',\n",
       " 'no',\n",
       " 'idea',\n",
       " 'what',\n",
       " 'I',\n",
       " 'did',\n",
       " 'USER_0',\n",
       " 'im',\n",
       " 'the',\n",
       " 'hungry',\n",
       " 'one',\n",
       " 'and',\n",
       " 'she',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'share',\n",
       " 'very',\n",
       " 'well',\n",
       " 'USER_0',\n",
       " 'haha',\n",
       " 'you',\n",
       " 'no',\n",
       " 'hes',\n",
       " 'sexy',\n",
       " 'USER_0',\n",
       " 'Aww',\n",
       " 'I',\n",
       " 'know',\n",
       " 'you',\n",
       " 'were',\n",
       " 'sorry',\n",
       " 'was',\n",
       " 'just',\n",
       " 'flustered',\n",
       " 'Thanks',\n",
       " 'for',\n",
       " 'helping',\n",
       " 'didn',\n",
       " 't',\n",
       " 'mean',\n",
       " 'to',\n",
       " 'throw',\n",
       " 'it',\n",
       " 'back',\n",
       " 'at',\n",
       " 'ya',\n",
       " 'feels',\n",
       " 'terrible',\n",
       " 'USER_0',\n",
       " 'USER_1',\n",
       " 'USER_2',\n",
       " 'still',\n",
       " 'no',\n",
       " 'shout',\n",
       " 'out',\n",
       " 'for',\n",
       " 'Spider',\n",
       " 'ive',\n",
       " 'used',\n",
       " 'titterfox',\n",
       " 'for',\n",
       " 'so',\n",
       " 'long',\n",
       " 'im',\n",
       " 'having',\n",
       " 'trouble',\n",
       " 'switching',\n",
       " 'to',\n",
       " 'tweetdeck',\n",
       " 'USER_0',\n",
       " 'I',\n",
       " 'love',\n",
       " 'to',\n",
       " 'laugh',\n",
       " 'from',\n",
       " 'mary',\n",
       " 'poppins',\n",
       " 'if',\n",
       " 'you',\n",
       " 'have',\n",
       " 'it',\n",
       " 'Thanks',\n",
       " 'USER_0',\n",
       " 'ooooh',\n",
       " 'i',\n",
       " 'd',\n",
       " 'pay',\n",
       " 'to',\n",
       " 'see',\n",
       " 'that',\n",
       " 'too',\n",
       " 'playing',\n",
       " 'in',\n",
       " 'a',\n",
       " 'cubby',\n",
       " 'house',\n",
       " 'USER_0',\n",
       " 'Wow',\n",
       " 'only',\n",
       " '2',\n",
       " 'weeks',\n",
       " 'until',\n",
       " 'the',\n",
       " 'new',\n",
       " 'kitchen',\n",
       " 'is',\n",
       " 'done',\n",
       " 'Exciting',\n",
       " 'Ouuchh',\n",
       " 'I',\n",
       " 'Just',\n",
       " 'Bit',\n",
       " 'my',\n",
       " 'tounge',\n",
       " 'USER_0',\n",
       " 'I',\n",
       " 'missed',\n",
       " 'mine',\n",
       " 'too',\n",
       " 'Congratulations',\n",
       " 'on',\n",
       " 'catching',\n",
       " 'yourself',\n",
       " 'in',\n",
       " 'time',\n",
       " 'tell',\n",
       " 'all',\n",
       " 'your',\n",
       " 'moms',\n",
       " 'happy',\n",
       " 'mothers',\n",
       " 'day',\n",
       " 'from',\n",
       " 'dave',\n",
       " '1337',\n",
       " 'Listening',\n",
       " 'to',\n",
       " 'AP',\n",
       " 'acoustic',\n",
       " 'sessions',\n",
       " 'with',\n",
       " 'nevershoutnever',\n",
       " 'Woke',\n",
       " 'up',\n",
       " 'at',\n",
       " '4AM',\n",
       " 'for',\n",
       " 'who',\n",
       " 'only',\n",
       " 'knows',\n",
       " 'what',\n",
       " 'reason',\n",
       " 'and',\n",
       " 'now',\n",
       " 'I',\n",
       " 'can',\n",
       " 't',\n",
       " 'fall',\n",
       " 'back',\n",
       " 'asleep',\n",
       " 'enjoyed',\n",
       " 'going',\n",
       " 'out',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sunshine',\n",
       " 'and',\n",
       " 'shopping',\n",
       " 'USER_0',\n",
       " 'send',\n",
       " 'the',\n",
       " 'rain',\n",
       " 'this',\n",
       " 'way',\n",
       " 'so',\n",
       " 'i',\n",
       " 'can',\n",
       " 'get',\n",
       " 'off',\n",
       " 'early',\n",
       " 'x3',\n",
       " 'bLaiNe',\n",
       " 'USER_0',\n",
       " 'You',\n",
       " 're',\n",
       " 'welcome',\n",
       " 'No',\n",
       " 'worries',\n",
       " 'it',\n",
       " 's',\n",
       " 'a',\n",
       " 'big',\n",
       " 'school',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'up',\n",
       " 'with',\n",
       " 'i',\n",
       " 'lost',\n",
       " 'my',\n",
       " 'book',\n",
       " 'while',\n",
       " 'i',\n",
       " 'was',\n",
       " 'sleeping',\n",
       " 'who',\n",
       " 'does',\n",
       " 'that',\n",
       " 'USER_0',\n",
       " 'nope',\n",
       " 'did',\n",
       " 'it',\n",
       " 'on',\n",
       " 'my',\n",
       " 'Windows',\n",
       " 'XP',\n",
       " 'Very',\n",
       " 'long',\n",
       " 'and',\n",
       " 'draining',\n",
       " 'process',\n",
       " 'though',\n",
       " 'On',\n",
       " 'the',\n",
       " 'way',\n",
       " 'to',\n",
       " 'metro',\n",
       " 'tv',\n",
       " 'don',\n",
       " 't',\n",
       " 'forget',\n",
       " 'to',\n",
       " 'see',\n",
       " 'SGU',\n",
       " 'dudes',\n",
       " 'at',\n",
       " 'Democrazy',\n",
       " 'show',\n",
       " 'on',\n",
       " 'metro',\n",
       " 'tv',\n",
       " 'I',\n",
       " 'think',\n",
       " 'i',\n",
       " 'am',\n",
       " 'weak',\n",
       " 'on',\n",
       " 'Twitter',\n",
       " 'boohaa',\n",
       " 'wishes',\n",
       " 'it',\n",
       " 'was',\n",
       " 'the',\n",
       " '17th',\n",
       " 'century',\n",
       " 'and',\n",
       " 'a',\n",
       " 'dashing',\n",
       " 'pirate',\n",
       " 'was',\n",
       " 'in',\n",
       " 'love',\n",
       " 'with',\n",
       " 'her',\n",
       " 'sigh',\n",
       " 'And',\n",
       " 'hello',\n",
       " 'to',\n",
       " 'all',\n",
       " 'my',\n",
       " 'new',\n",
       " 'followers',\n",
       " 'Going',\n",
       " 'to',\n",
       " 'bed',\n",
       " 'watching',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'qi',\n",
       " 'Wake',\n",
       " 'up',\n",
       " 'tomorrow',\n",
       " 'to',\n",
       " 'face',\n",
       " 'the',\n",
       " 'start',\n",
       " 'of',\n",
       " 'my',\n",
       " 'last',\n",
       " 'summer',\n",
       " 'in',\n",
       " 'bath',\n",
       " 'Bit',\n",
       " 'gutted',\n",
       " 'atm',\n",
       " 'to',\n",
       " 'be',\n",
       " 'home',\n",
       " 'is',\n",
       " 'listening',\n",
       " 'to',\n",
       " 'The',\n",
       " 'RAMONES',\n",
       " 'USER_0',\n",
       " 'GoodLuck',\n",
       " 'Wishing',\n",
       " 'I',\n",
       " 'could',\n",
       " 'be',\n",
       " 'in',\n",
       " 'NOLA',\n",
       " 'this',\n",
       " 'weekend',\n",
       " 'oh',\n",
       " 'well',\n",
       " 'I',\n",
       " 'll',\n",
       " 'be',\n",
       " 'there',\n",
       " 'Tuesday',\n",
       " 'crap',\n",
       " 'i',\n",
       " 'really',\n",
       " 'dont',\n",
       " 'like',\n",
       " 'the',\n",
       " 'twitter',\n",
       " 'hit',\n",
       " 'thingy',\n",
       " 'dont',\n",
       " 'use',\n",
       " 'it',\n",
       " 'USER_0',\n",
       " 'No',\n",
       " 'recipes',\n",
       " 'on',\n",
       " 'the',\n",
       " 'new',\n",
       " 'website',\n",
       " 'USER_0',\n",
       " 'you',\n",
       " 'spelled',\n",
       " 'cheesecake',\n",
       " 'wrong',\n",
       " 'soooo',\n",
       " 'ya',\n",
       " 'girl',\n",
       " 'got',\n",
       " 'into',\n",
       " 'Long',\n",
       " 'Beach',\n",
       " 'State',\n",
       " 's',\n",
       " 'Master',\n",
       " 's',\n",
       " 'of',\n",
       " 'Social',\n",
       " 'Work',\n",
       " 'program',\n",
       " 'HOLLA',\n",
       " 'God',\n",
       " 'is',\n",
       " 'good',\n",
       " 'my',\n",
       " 'daddy',\n",
       " 's',\n",
       " 'a',\n",
       " 'G',\n",
       " 'this',\n",
       " 'is',\n",
       " 'why',\n",
       " 'i',\n",
       " 'm',\n",
       " 'hott',\n",
       " 'Elections',\n",
       " 'have',\n",
       " 'consequences',\n",
       " 'ours',\n",
       " 'sure',\n",
       " 'did',\n",
       " 'USER_0',\n",
       " 'Paranoid',\n",
       " 'is',\n",
       " 'amazing',\n",
       " 'I',\n",
       " 'can',\n",
       " 't',\n",
       " 'wait',\n",
       " 'to',\n",
       " 'hear',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'your',\n",
       " 'album',\n",
       " 'USER_0',\n",
       " 'David',\n",
       " 'Lynch',\n",
       " 'is',\n",
       " 'a',\n",
       " 'huge',\n",
       " 'inspiration',\n",
       " 'of',\n",
       " 'mine',\n",
       " 'so',\n",
       " 'I',\n",
       " 'guess',\n",
       " 'yes',\n",
       " 'there',\n",
       " 'is',\n",
       " 'a',\n",
       " 'similar',\n",
       " 'element',\n",
       " 'of',\n",
       " 'mystery',\n",
       " 'within',\n",
       " 'them',\n",
       " 'Maybe',\n",
       " 'I',\n",
       " 'll',\n",
       " 'go',\n",
       " 'for',\n",
       " 'the',\n",
       " 'Nokia',\n",
       " '5800',\n",
       " 'is',\n",
       " 'not',\n",
       " 'ipod',\n",
       " 'and',\n",
       " 'has',\n",
       " 'wifi',\n",
       " 'seems',\n",
       " 'like',\n",
       " 'no',\n",
       " 'leaks',\n",
       " 'today',\n",
       " 'we',\n",
       " 'shall',\n",
       " 'wait',\n",
       " 'a',\n",
       " 'few',\n",
       " 'more',\n",
       " 'days',\n",
       " 'for',\n",
       " 'final',\n",
       " 'verdict',\n",
       " 'and',\n",
       " 'still',\n",
       " 'no',\n",
       " 'tennis',\n",
       " 'call',\n",
       " 'yet',\n",
       " 'ughhhhhhhhhhhh',\n",
       " 'i',\n",
       " 'wish',\n",
       " 'i',\n",
       " 'could',\n",
       " 'see',\n",
       " 'lovehatehero',\n",
       " 'tonight',\n",
       " 'THEY',\n",
       " 'ARE',\n",
       " 'IN',\n",
       " 'PALMDALE',\n",
       " 'ughhhhhhhhh',\n",
       " 'im',\n",
       " 'stilll',\n",
       " 'pissed',\n",
       " 'about',\n",
       " 'that',\n",
       " 'USER_0',\n",
       " 'start',\n",
       " 'stufying',\n",
       " 'after',\n",
       " 'lunch',\n",
       " 'and',\n",
       " 'loose',\n",
       " 'woman',\n",
       " 'a',\n",
       " 'pink',\n",
       " 'balloon',\n",
       " 'for',\n",
       " 'mum',\n",
       " 'on',\n",
       " 'her',\n",
       " 'day',\n",
       " 'USER_0',\n",
       " 'USER_1',\n",
       " 'I',\n",
       " 'want',\n",
       " 'in',\n",
       " 'on',\n",
       " 'that',\n",
       " 'too',\n",
       " 'USER_0',\n",
       " 'you',\n",
       " 'lucky',\n",
       " 'so',\n",
       " 'and',\n",
       " 'so',\n",
       " 'Have',\n",
       " 'you',\n",
       " 'got',\n",
       " 'IT',\n",
       " 'on',\n",
       " 'Friday',\n",
       " 'S',\n",
       " 'pose',\n",
       " 'you',\n",
       " 've',\n",
       " 'got',\n",
       " 'a',\n",
       " 'lie',\n",
       " 'in',\n",
       " 'on',\n",
       " 'Monday',\n",
       " 'too',\n",
       " 'unless',\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'Welsh',\n",
       " 'exam',\n",
       " 'Adrianna',\n",
       " 'said',\n",
       " 'don',\n",
       " 't',\n",
       " 'get',\n",
       " 'a',\n",
       " 'kitty',\n",
       " 'booo',\n",
       " 'that',\n",
       " 'whore',\n",
       " 'USER_0',\n",
       " 'I',\n",
       " 'made',\n",
       " 'Mally',\n",
       " 'a',\n",
       " 'blonde',\n",
       " 'brown',\n",
       " 'diva',\n",
       " 'doll',\n",
       " 'and',\n",
       " 'the',\n",
       " 'moment',\n",
       " 'she',\n",
       " 'got',\n",
       " 'it',\n",
       " 'she',\n",
       " 'made',\n",
       " 'it',\n",
       " 'red',\n",
       " 'blue',\n",
       " 'and',\n",
       " 'put',\n",
       " 'on',\n",
       " 'too',\n",
       " 'much',\n",
       " 'eyeliner',\n",
       " 'I',\n",
       " 'have',\n",
       " 'a',\n",
       " 'headach',\n",
       " 'not',\n",
       " 'going',\n",
       " 'out',\n",
       " 'tonight',\n",
       " 'USER_0',\n",
       " 'ahhhhh',\n",
       " 'i',\n",
       " 'remember',\n",
       " 'that',\n",
       " 'poem',\n",
       " 'love',\n",
       " 'it',\n",
       " 'Cry',\n",
       " 'is',\n",
       " 'wht',\n",
       " 'I',\n",
       " 'can',\n",
       " 'do',\n",
       " 'why',\n",
       " 'u',\n",
       " 'do',\n",
       " 'that',\n",
       " 'to',\n",
       " 'me',\n",
       " 'URL_0',\n",
       " 'USER_0',\n",
       " 'who',\n",
       " 'who',\n",
       " 'are',\n",
       " 'you',\n",
       " 'gonna',\n",
       " 'watch',\n",
       " 'Josh',\n",
       " 'Groban',\n",
       " 'who',\n",
       " 'jaycee',\n",
       " 'jam',\n",
       " 'is',\n",
       " 'now',\n",
       " 'renamed',\n",
       " 'jaycee',\n",
       " 'syrup',\n",
       " 'it',\n",
       " 'didn',\n",
       " 't',\n",
       " 'set',\n",
       " 'I',\n",
       " 'think',\n",
       " 'we',\n",
       " 'have',\n",
       " 'everything',\n",
       " 'we',\n",
       " 'need',\n",
       " 'now',\n",
       " 'Time',\n",
       " 'to',\n",
       " 'finish',\n",
       " 'the',\n",
       " 'bathroom',\n",
       " 'Hopefully',\n",
       " 'that',\n",
       " 'was',\n",
       " 'our',\n",
       " 'last',\n",
       " 'trip',\n",
       " 'to',\n",
       " 'The',\n",
       " 'Home',\n",
       " 'Depot',\n",
       " 'my',\n",
       " 'hair',\n",
       " 'is',\n",
       " 'all',\n",
       " 'poofy',\n",
       " 'it',\n",
       " 's',\n",
       " 'weird',\n",
       " 'USER_0',\n",
       " 'your',\n",
       " 'nyx',\n",
       " 'haul',\n",
       " 'looks',\n",
       " 'amazing',\n",
       " 'lip',\n",
       " 'swatches',\n",
       " 'please',\n",
       " 'i',\n",
       " 'want',\n",
       " 'to',\n",
       " 'see',\n",
       " 'how',\n",
       " 'they',\n",
       " 'look',\n",
       " 'like',\n",
       " 'on',\n",
       " 'USER_0',\n",
       " 'Hey',\n",
       " 'Love',\n",
       " 'Jenn',\n",
       " 'please',\n",
       " 'say',\n",
       " 'something',\n",
       " 'to',\n",
       " 'me',\n",
       " 'anything',\n",
       " 'at',\n",
       " 'all',\n",
       " 'I',\n",
       " 'dont',\n",
       " 'want',\n",
       " 'to',\n",
       " 'bother',\n",
       " 'you',\n",
       " 'and',\n",
       " 'Im',\n",
       " 'sorry',\n",
       " 'for',\n",
       " 'anything',\n",
       " 'I',\n",
       " 'did',\n",
       " 'please',\n",
       " 'say',\n",
       " 'something',\n",
       " 'please',\n",
       " 'I',\n",
       " 'been',\n",
       " 'sleeping',\n",
       " 'all',\n",
       " 'day',\n",
       " 'my',\n",
       " 'leg',\n",
       " 'hurts',\n",
       " 'from',\n",
       " 'roller',\n",
       " 'skating',\n",
       " 'I',\n",
       " 'm',\n",
       " 'bruised',\n",
       " 'USER_0',\n",
       " 'Yea',\n",
       " 'creeper',\n",
       " 'Finally',\n",
       " 'last',\n",
       " 'week',\n",
       " 'of',\n",
       " 'school',\n",
       " 'Please',\n",
       " 'let',\n",
       " 'this',\n",
       " 'week',\n",
       " 'go',\n",
       " 'by',\n",
       " 'fast',\n",
       " 'USER_0',\n",
       " 'and',\n",
       " 'for',\n",
       " 'later',\n",
       " 'in',\n",
       " 'life',\n",
       " 'too',\n",
       " 'USER_0',\n",
       " 'LOL',\n",
       " 'what',\n",
       " 'whyyy',\n",
       " 'that',\n",
       " 's',\n",
       " 'lame',\n",
       " 'directing',\n",
       " 'a',\n",
       " 'theatre',\n",
       " 'play',\n",
       " 'USER_0',\n",
       " 'LOL',\n",
       " 'none',\n",
       " 'My',\n",
       " 'dad',\n",
       " 'doesn',\n",
       " 't',\n",
       " 'send',\n",
       " 'those',\n",
       " 'They',\n",
       " 're',\n",
       " 'all',\n",
       " 'short',\n",
       " 'one',\n",
       " 'or',\n",
       " 'two',\n",
       " 'liners',\n",
       " 'If',\n",
       " 'he',\n",
       " 'were',\n",
       " 'a',\n",
       " 'texter',\n",
       " 'he',\n",
       " 'd',\n",
       " 'text',\n",
       " 'them',\n",
       " 'Wow',\n",
       " 'today',\n",
       " 'was',\n",
       " 'pretty',\n",
       " 'good',\n",
       " 'until',\n",
       " 'I',\n",
       " 'got',\n",
       " 'home',\n",
       " 'Maybe',\n",
       " 'I',\n",
       " 'need',\n",
       " 'to',\n",
       " 'start',\n",
       " 'working',\n",
       " 'late',\n",
       " 'more',\n",
       " 'USER_0',\n",
       " 'Are',\n",
       " 'you',\n",
       " 'okay',\n",
       " 'Jodie',\n",
       " 'Love',\n",
       " 'that',\n",
       " 'acoustic',\n",
       " 'version',\n",
       " 'by',\n",
       " 'the',\n",
       " 'way',\n",
       " 'has',\n",
       " 'just',\n",
       " 'downloaded',\n",
       " 'jennette',\n",
       " 'mccurdy',\n",
       " 'so',\n",
       " 'close',\n",
       " 'homeless',\n",
       " 'heart',\n",
       " 'WOO',\n",
       " 'USER_0',\n",
       " 'for',\n",
       " 'the',\n",
       " 'love',\n",
       " 'of',\n",
       " 'god',\n",
       " 'n√É',\n",
       " 'o',\n",
       " 'ishtudei',\n",
       " 'puurrrcausa',\n",
       " 'do',\n",
       " 'fisl',\n",
       " 'USER_0',\n",
       " 'walshyyy',\n",
       " 'got',\n",
       " 'twittter',\n",
       " 'USER_0',\n",
       " 'more',\n",
       " 'teddy',\n",
       " 'bear',\n",
       " 'shaped',\n",
       " 'They',\n",
       " 'kept',\n",
       " 'sticking',\n",
       " 'to',\n",
       " 'my',\n",
       " 'hands',\n",
       " 'USER_0',\n",
       " 'lucky',\n",
       " 'yesterday',\n",
       " 'it',\n",
       " 'was',\n",
       " '27',\n",
       " 'here',\n",
       " 'Haven',\n",
       " 't',\n",
       " 'worked',\n",
       " 'out',\n",
       " 'since',\n",
       " 'last',\n",
       " 'week',\n",
       " 'and',\n",
       " 'haven',\n",
       " 't',\n",
       " 'really',\n",
       " 'been',\n",
       " 'eating',\n",
       " 'healthy',\n",
       " 'URL_0',\n",
       " 'USER_0',\n",
       " 'I',\n",
       " 'haven',\n",
       " 't',\n",
       " 'tried',\n",
       " 'Twittelator',\n",
       " 'Pro',\n",
       " 'Secretly',\n",
       " 'hoping',\n",
       " 'I',\n",
       " 'don',\n",
       " 't',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a dictionary {token: token_id} of 50000 most frequent tokens.\n",
    "\n",
    "After that, make an inverse dictionary {token_id: token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(tokens, max_size=20000):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary of at most max_size words from the supplied list of lists of tokens.\n",
    "    If a word embedding model is provided, adds only the words present in the model vocabulary.\n",
    "    \"\"\"\n",
    "    vocabulary = {}\n",
    "    reserved_symbols = [\"NULL\", \"UNKN\"]\n",
    "    \n",
    "    counter = collections.Counter(tokens)\n",
    "    \n",
    "    freq_toks = counter.most_common(max_size-len(reserved_symbols))\n",
    "\n",
    "    voc_words = [k[0] for k in freq_toks]\n",
    "\n",
    "    for i, reserved in enumerate(reserved_symbols):\n",
    "        vocabulary[reserved] = i\n",
    "\n",
    "    for i, k in enumerate(voc_words):\n",
    "        vocabulary[k] = i+len(reserved_symbols)\n",
    "\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = build_vocabulary(tokens, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "reverse_dictionary = {v:k for k,v in dictionary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "assert len(dictionary) == len(reverse_dictionary) == 50000\n",
    "assert sorted(dictionary.keys()) == sorted(reverse_dictionary.values())\n",
    "assert sorted(reverse_dictionary.keys()) == sorted(dictionary.values())\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the tokens into a list of their identifiers from your 'dictionary'. Replace the Out Of Vocabulary [OOV] tokens with 'UNKN' identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(tokens, token_to_id):\n",
    "    # your code goes here\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = encode(tokens, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Sample encoded data', data[:5])\n",
    "print('Sample decoded data', [reverse_dictionary[t] for t in data[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t, tid in zip(tokens, data):\n",
    "    assert ((reverse_dictionary[tid] == t) or (tid==dictionary['UNKN']))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are done with data preparation, now train the word2vec model. You don't need to change anything in this section.\n",
    "\n",
    "Read the comments to get a better understanding of what is happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1 # [ skip_window target skip_window ]\n",
    "    # Initilzing a buffer to sample from\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Moving over the the corpus to the next buffer while we get a whole batch\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # set target somewehre, then find the right ones\n",
    "        targets_to_avoid = [ skip_window ]  #don't use the center word as a target\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "# Let's test how this works\n",
    "for num_skips, skip_window in [(2, 1)]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch(batch_size=8, num_skips=num_skips, skip_window=skip_window)\n",
    "    print('\\nwith num_skips = %d and skip_window = %d:' % (num_skips, skip_window))\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow word2vec computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "skip_window = 1 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a label.\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "    # Input data.\n",
    "    train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "    # Variables.\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    softmax_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Model.\n",
    "    # Look up embeddings for inputs.\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "    # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(weights=softmax_weights, biases=softmax_biases, inputs=embed,\n",
    "                                   labels=train_labels, num_sampled=num_sampled, num_classes=vocabulary_size))\n",
    "\n",
    "    # Optimizer.\n",
    "    # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "    # This is because the embeddings are defined as a variable quantity and the\n",
    "    # optimizer's `minimize` method will by default modify all variable quantities \n",
    "    # that contribute to the tensor it is passed.\n",
    "    # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "    optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)\n",
    "  \n",
    "    # Compute the similarity between minibatch examples and all embeddings.\n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 100001\n",
    "lh = []\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batch_data, batch_labels = generate_batch(\n",
    "          batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "            display.clear_output(wait=True)\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step %d: %f' % (step, average_loss))\n",
    "            lh.append(average_loss)\n",
    "            average_loss = 0\n",
    "\n",
    "            plt.figure(figsize=(14, 10))\n",
    "\n",
    "            plt.title(\"Training loss, step {}/{}\".format(step, num_steps))\n",
    "            plt.xlabel(\"#step\")\n",
    "            plt.ylabel(\"loss\")\n",
    "            plt.plot(lh, 'b')\n",
    "            plt.show()\n",
    "\n",
    "        if step % 2000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8 # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                log = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log = '%s %s,' % (log, close_word)\n",
    "                print(log)\n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the word vectors and dictionaries for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vec_file(final_emb_mtx, vocab_size, vec_size,filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(str(vocab_size)+' '+str(vec_size) + '\\n')\n",
    "        for n in range(vocab_size):\n",
    "            s = ' '.join([reverse_dictionary[n]] + [str(num) for num in final_emb_mtx[n]])\n",
    "            f.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "create_vec_file(final_embeddings, vocab_size=vocabulary_size, vec_size=embedding_size, filename='simple_cbow.w2v')\n",
    "pickle.dump([dictionary, reverse_dictionary], open('dict_rdict.pkl',\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Find the closest words to a given word using cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_closest_words(word, embeddings, dictionary, reverse_dictionary, k=3):\n",
    "    # Find the similarity between all the word vectors and the use dictionary to return the exact words\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Try some algebra on the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The idea is to write a function that would show us the mom-dad+father==mother and went-go+eat=ate relationships\n",
    "def find_related_word(source_word, target_words, new_target_word_form,\n",
    "                      embeddings, dictionary, reverse_dictionary):\n",
    "    return new_source_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T_SNE projection of word vectors into a 2-dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dd = json.load(open('topical_words.json'))\n",
    "colors = {'colors':'r', 'vegetables':'g', 'numbers':'b', 'professions': 'GoldenRod'}\n",
    "wd = {}\n",
    "for k in dd:\n",
    "    for w in dd[k]:\n",
    "        if w in dictionary:\n",
    "            wd[w] = {'emb': final_embeddings[dictionary[w]], 'label': k, 'color': colors[k]}\n",
    "            \n",
    "M = np.array([wd[k]['emb'] for k in wd])\n",
    "labels = [wd[k]['label'] for k in wd]\n",
    "colors = [wd[k]['color'] for k in wd]\n",
    "\n",
    "P = squareform(pdist(M, metric='cosine'))\n",
    "tsne2 = TSNE(n_components=2, random_state=34, metric='precomputed', n_iter=9001)\n",
    "Y = tsne2.fit_transform(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for i, k in enumerate(wd):\n",
    "    plt.scatter([Y[i][0]], [Y[i][1]], color = wd[k]['color'], label=wd[k]['label'])\n",
    "    plt.annotate(k, xy=(Y[i][0], Y[i][1]), xytext=(0, 0), textcoords='offset points', \n",
    "                 color=wd[k]['color'], fontsize=12)\n",
    "\n",
    "\n",
    "plt.title('T-SNE on word2vec representations, 4 topics', fontsize=16)        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
