{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Terms\n",
    "### For all of the problems I have provided code that works on my implementation. For many of you this should work immediately if you have done all the instructions right. However for some of you, you mmight need some small adjustments for these to work. It should not be hard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deadline?  Tomorrow, Tuesday 1pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to submit.\n",
    "## You need to submit one zip file with all of your notebooks for PA1-4. We should be able to run your notebook by donig run all cells, and then we should see all the results wanted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where to submit and how\n",
    "\n",
    "# Send email to\n",
    "## ml.homeworks.aca+PA1-4@gmail.com\n",
    "### With your subject line \"Full Submission For PA1-4\"\n",
    "## Each of your notebooks should have the name \"FullName, PA_xPart_y\"\n",
    "## Your Zip file should have name \"FullNamePA1-4Submission\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA 4 Deliverables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit two notebooks, one for Part 1 and one for Part 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit your gradient descent to also max_count, max_cost_before_break\n",
    "### Important. Do not end your gradient descent if cost difference is smaller than eps.\n",
    "## Only use the cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cost_function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6686e692935e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Add this in the while cycle of your gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m## This should be run for every cnt, not just %100=0 or something like that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_beta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mcosts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cost_function' is not defined"
     ]
    }
   ],
   "source": [
    "## Add this in the while cycle of your gradient descent \n",
    "## This should be run for every cnt, not just %100=0 or something like that \n",
    "cost = cost_function(current_beta, X,y,lam)\n",
    "costs[cnt] = cost\n",
    "end = time.time()\n",
    "if int(end-begin) not in times:\n",
    "    times[int(end-begin)] = cnt\n",
    "    print (\"%s seconds has passed at count %s out of %s\" % (end-begin, cnt, max_count))\n",
    "if cost> max_cost_before_break:\n",
    "    break\n",
    "if cnt>max_count:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add this in the beginning of your gradient descent function\n",
    "times = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run your notebook it should have cells with following and give similar results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = -15\n",
    "xmax = 20\n",
    "d_star = 1\n",
    "d_train = 1\n",
    "beta_star = [1, 1, 0.025, - 0.005]\n",
    "sigma = 2\n",
    "num_points = 10000\n",
    "lam = 0\n",
    "max_count = 10000\n",
    "x, y = generate_points_polynomial(xmin, xmax, num_points, sigma,  [1,1], d_star)\n",
    "X = transform_x(x,d_train)\n",
    "alpha_list = [0.01,0.001, 0.0001]\n",
    "\n",
    "\n",
    "for alpha in alpha_list:\n",
    "    begin = time.time()\n",
    "    print (\"Starting gradient descent for alpha=%s\" % alpha)\n",
    "   \n",
    "    \n",
    "    beta_grad, costs = regression_gradient_descent(X,y,lam, alpha,max_count,max_cost_before_break)\n",
    "    end = time.time()\n",
    "    print (\"Execution time =%s\" % (end-begin))\n",
    "    print (\"Beta_grad COST is=%s\\n\" % cost_function(beta_grad,X,y,lam))\n",
    "    plt.figure(1)\n",
    "\n",
    "    plt.loglog(costs.keys(),costs.values(), label=\"alpha=%s and best_cost=%.2lf\" % (alpha, min(costs.values())))\n",
    "    plt.figure(2)\n",
    "\n",
    "    plt.semilogy(costs.keys(),costs.values(), label=\"alpha=%s and best_cost=%.2lf\" % (alpha, min(costs.values())))\n",
    "plt.figure(1)\n",
    "plt.legend()\n",
    "plt.title(\"Learning curve, NO scaling,loglog, d_star=%s, sigma=%s,\\n \\\n",
    "d_train=%s, numpoints=%s, starting beta = ZEROS\" % (d_star,sigma,d_train, num_points))\n",
    "\n",
    "plt.xlabel(\"Number of steps\")\n",
    "plt.ylabel(\"Cost Function\")\n",
    "plt.figure(2)\n",
    "plt.title(\"Learning curve, NO scaling,Semilog, d_star=%s, sigma=%s,\\n \\\n",
    "d_train=%s, numpoints=%s, starting beta = ZEROS\" % (d_star,sigma,d_train, num_points))\n",
    "\n",
    "plt.xlabel(\"Number of steps\")\n",
    "plt.ylabel(\"Cost Function\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](4.1.3.png) \n",
    "![title](4.1.4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it should also have this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = -15\n",
    "xmax = 20\n",
    "d_star = 3\n",
    "d_train = 3\n",
    "beta_star = [1, 1, 0.025, - 0.005]\n",
    "sigma = 2\n",
    "num_points = 1000\n",
    "lam = 0\n",
    "max_count = 300000\n",
    "max_cost_before_break = 1000\n",
    "x, y = generate_points_polynomial(xmin, xmax, num_points, sigma,  beta_star, d_star)\n",
    "X = transform_x(x,d_train)\n",
    "alpha_list = [0.0000004,0.0000001,0.00000025]\n",
    "\n",
    "\n",
    "for alpha in alpha_list:\n",
    "    begin = time.time()\n",
    "    print (\"Starting gradient descent for alpha=%s\" % alpha)\n",
    "   \n",
    "    \n",
    "    beta_grad, costs = regression_gradient_descent(X,y,lam, alpha,max_count, max_cost_before_break)\n",
    "    end = time.time()\n",
    "    print (\"Execution time =%s\" % (end-begin))\n",
    "    print (\"Beta_grad COST is=%s\\n\" % cost_function(beta_grad,X,y,lam))\n",
    "    plt.figure(1)\n",
    "\n",
    "    plt.loglog(costs.keys(),costs.values(), label=\"alpha=%s and best_cost=%.2lf\" % (alpha, min(costs.values())))\n",
    "    plt.figure(2)\n",
    "\n",
    "    plt.semilogy(costs.keys(),costs.values(), label=\"alpha=%s and best_cost=%.2lf\" % (alpha, min(costs.values())))\n",
    "plt.figure(1)\n",
    "plt.legend()\n",
    "plt.title(\"Learning curve, NO scaling,loglog, d_star=%s, sigma=%s,\\n \\\n",
    "d_train=%s, numpoints=%s, starting beta = ZEROS\" % (d_star,sigma,d_train, num_points))\n",
    "\n",
    "plt.xlabel(\"Number of steps\")\n",
    "plt.ylabel(\"Cost Function\")\n",
    "plt.figure(2)\n",
    "plt.title(\"Learning curve, NO scaling,Semilog, d_star=%s, sigma=%s,\\n \\\n",
    "d_train=%s, numpoints=%s, starting beta = ZEROS\" % (d_star,sigma,d_train, num_points))\n",
    "\n",
    "plt.xlabel(\"Number of steps\")\n",
    "plt.ylabel(\"Cost Function\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](4.1.1.png) \n",
    "![title](4.1.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Part2 STARTING Beta for gradient descent has to be [2,2,...,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = -15\n",
    "xmax = 20\n",
    "d_star = 3\n",
    "d_train = 3\n",
    "beta_star = [1, 1, 0.025, - 0.005]\n",
    "sigma = 2\n",
    "num_points = 1000\n",
    "lam = 0\n",
    "max_count = 20000\n",
    "max_cost_before_break = 1000\n",
    "x, y = generate_points_polynomial(xmin, xmax, num_points, sigma,  beta_star, d_star)\n",
    "X = transform_x(x,d_train)\n",
    "alpha_list = [0.005, 0.01, 0.02, 0.1, 1]\n",
    "\n",
    "\n",
    "\n",
    "for alpha in alpha_list:\n",
    "    begin = time.time()\n",
    "    print (\"Starting gradient descent for alpha=%s\" % alpha)\n",
    "   \n",
    "    \n",
    "    beta_grad, costs = regression_scaled_gradient_descent(X,y,lam, alpha,max_count, max_cost_before_break)\n",
    "    end = time.time()\n",
    "    print (\"Execution time =%s\" % (end-begin))\n",
    "    print (\"Beta Grad is %s\" % beta_grad)\n",
    "    print (\"Beta_grad COST is=%s\\n\" % cost_function(beta_grad,X,y,lam))\n",
    "    plt.figure(1)\n",
    "\n",
    "    plt.loglog(costs.keys(),costs.values(), label=\"alpha=%s and best_cost=%.2lf\" % (alpha, min(costs.values())))\n",
    "    plt.figure(2)\n",
    "\n",
    "    plt.semilogy(costs.keys(),costs.values(), label=\"alpha=%s and best_cost=%.2lf\" % (alpha, min(costs.values())))\n",
    "plt.figure(1)\n",
    "plt.legend()\n",
    "plt.title(\"Learning curve, NO scaling,loglog, d_star=%s, sigma=%s,\\n \\\n",
    "d_train=%s, numpoints=%s, starting beta = ZEROS\" % (d_star,sigma,d_train, num_points))\n",
    "\n",
    "plt.xlabel(\"Number of steps\")\n",
    "plt.ylabel(\"Cost Function\")\n",
    "plt.figure(2)\n",
    "plt.title(\"Learning curve, NO scaling,Semilog, d_star=%s, sigma=%s,\\n \\\n",
    "d_train=%s, numpoints=%s, starting beta = ZEROS\" % (d_star,sigma,d_train, num_points))\n",
    "\n",
    "plt.xlabel(\"Number of steps\")\n",
    "plt.ylabel(\"Cost Function\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](4.2.1.png) \n",
    "![title](4.2.2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = -15\n",
    "xmax = 20\n",
    "d_star = 3\n",
    "d_train = 100\n",
    "beta_star = [1, 1, 0.025, - 0.005]\n",
    "sigma = 2\n",
    "num_points = 10000\n",
    "lam = 0\n",
    "max_count = 3000\n",
    "max_cost_before_break = 100000\n",
    "x, y = generate_points_polynomial(xmin, xmax, num_points, sigma,  beta_star, d_star)\n",
    "X = transform_x(x,d_train)\n",
    "alpha_list = [0.001, 0.003, 0.01,0.02, 0.03]\n",
    "\n",
    "\n",
    "\n",
    "for alpha in alpha_list:\n",
    "    begin = time.time()\n",
    "    print (\"Starting gradient descent for alpha=%s\" % alpha)\n",
    "   \n",
    "    beta_grad, costs = regression_scaled_gradient_descent(X,y,lam, alpha,max_count, max_cost_before_break)\n",
    "    end = time.time()\n",
    "    print (\"Execution time =%s\" % (end-begin))\n",
    "    print (\"Beta_grad COST is=%s\\n\" % cost_function(beta_grad,X,y,lam))\n",
    "    plt.figure(1)\n",
    "\n",
    "    plt.loglog(costs.keys(),costs.values(), label=\"alpha=%s and best_cost=%.2lf\" % (alpha, min(costs.values())))\n",
    "    plt.figure(2)\n",
    "\n",
    "    plt.semilogy(costs.keys(),costs.values(), label=\"alpha=%s and best_cost=%.2lf\" % (alpha, min(costs.values())))\n",
    "plt.figure(1)\n",
    "plt.legend()\n",
    "plt.title(\"Learning curve, WITH scaling,loglog, d_star=%s, sigma=%s,\\n \\\n",
    "d_train=%s, numpoints=%s, starting beta = 2*np.ones()\" % (d_star,sigma,d_train, num_points))\n",
    "\n",
    "plt.xlabel(\"Number of steps\")\n",
    "plt.ylabel(\"Cost Function\")\n",
    "plt.figure(2)\n",
    "plt.title(\"Learning curve, WITH scaling,Semilog, d_star=%s, sigma=%s,\\n \\\n",
    "d_train=%s, numpoints=%s, starting beta = 2*np.ones()\" % (d_star,sigma,d_train, num_points))\n",
    "\n",
    "plt.xlabel(\"Number of steps\")\n",
    "plt.ylabel(\"Cost Function\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](4.2.3.png) \n",
    "![title](4.2.4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
