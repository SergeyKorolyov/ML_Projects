{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Denoising Autoencoders And Where To Find Them (5 points)\n",
    "\n",
    "Today we're going to train deep autoencoders and deploy them to faces and search for similar images.\n",
    "\n",
    "Our new test subjects are human faces from the [lfw dataset](http://vis-www.cs.umass.edu/lfw/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images not found, donwloading...\n",
      "extracting...\n",
      "done\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8e62a227e228>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlfw_dataset\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfetch_lfw_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_lfw_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muse_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdimx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m38\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdimy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m38\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m256.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\Scripts\\1_Notebook\\ACA\\ML\\Supervised learning (Intro to ML and NN)\\ACA2018-master\\lfw_dataset.py\u001b[0m in \u001b[0;36mfetch_lfw_dataset\u001b[1;34m(attrs_name, images_name, raw_images_name, use_raw, dx, dy, dimx, dimy)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tar xvzf tmp.tgz && rm tmp.tgz\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"done\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_images_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattrs_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from lfw_dataset import fetch_lfw_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, attr = fetch_lfw_dataset(use_raw=True,dimx=38,dimy=38)\n",
    "X = X.astype('float32') / 256.0\n",
    "\n",
    "img_shape = X.shape[1:]\n",
    "\n",
    "X_train, X_test = train_test_split(X, test_size=0.1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('sample image')\n",
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(X[i])\n",
    "\n",
    "print(\"X shape:\",X.shape)\n",
    "print(\"attr shape:\",attr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder architecture\n",
    "\n",
    "Let's design autoencoder as two sequential keras models: the encoder and decoder respectively.\n",
    "\n",
    "We will then use symbolic API to apply and train those models.\n",
    "\n",
    "<img src=\"http://nghiaho.com/wp-content/uploads/2012/12/autoencoder_network1.png\" width=640px>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras, keras.layers as L\n",
    "s = keras.backend.get_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First step: PCA\n",
    "\n",
    "Principial Component Analysis is a popular dimensionality reduction method. \n",
    "\n",
    "Under the hood, PCA attempts to decompose object-feature matrix $X$ into two smaller matrices: $W$ and $\\hat W$ minimizing _mean squared error_:\n",
    "\n",
    "$$\\|(X W) \\hat{W} - X\\|^2_2 \\to_{W, \\hat{W}} \\min$$\n",
    "- $X \\in \\mathbb{R}^{n \\times m}$ - object matrix (**centered**);\n",
    "- $W \\in \\mathbb{R}^{m \\times d}$ - matrix of direct transformation;\n",
    "- $\\hat{W} \\in \\mathbb{R}^{d \\times m}$ - matrix of reverse transformation;\n",
    "- $n$ samples, $m$ original dimensions and $d$ target dimensions;\n",
    "\n",
    "In geometric terms, we want to find d axes along which most of variance occurs. The \"natural\" axes, if you wish.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/9/90/PCA_fish.png/256px-PCA_fish.png)\n",
    "\n",
    "\n",
    "PCA can also be seen as a special case of an autoencoder.\n",
    "\n",
    "* __Encoder__: X -> Dense(d units) -> code\n",
    "* __Decoder__: code -> Dense(m units) -> X\n",
    "\n",
    "Where Dense is a fully-connected layer with linear activaton:   $f(X) = W \\cdot X + \\vec b $\n",
    "\n",
    "\n",
    "Note: the bias term in those layers is responsible for \"centering\" the matrix i.e. substracting mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_pca_autoencoder(img_shape,code_size=32):\n",
    "    \"\"\"\n",
    "    Here we define a simple linear autoencoder as described above.\n",
    "    We also flatten and un-flatten data to be compatible with image shapes\n",
    "    \"\"\"\n",
    "    \n",
    "    encoder = keras.models.Sequential()\n",
    "    encoder.add(L.InputLayer(img_shape))\n",
    "    encoder.add(L.Flatten())                  #flatten image to vector\n",
    "    encoder.add(L.Dense(code_size))           #actual encoder\n",
    "\n",
    "    decoder = keras.models.Sequential()\n",
    "    decoder.add(L.InputLayer((code_size,)))\n",
    "    decoder.add(L.Dense(np.prod(img_shape)))  #actual decoder, height*width*3 units\n",
    "    decoder.add(L.Reshape(img_shape))         #un-flatten\n",
    "    \n",
    "    return encoder,decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meld them together into one model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder,decoder = build_pca_autoencoder(img_shape,code_size=32)\n",
    "\n",
    "inp = L.Input(img_shape)\n",
    "code = encoder(inp)\n",
    "reconstruction = decoder(code)\n",
    "\n",
    "autoencoder = keras.models.Model(inp,reconstruction)\n",
    "autoencoder.compile('adamax','mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(x=X_train,y=X_train,epochs=32,\n",
    "                validation_data=[X_test,X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize(img,encoder,decoder):\n",
    "    \"\"\"Draws original, encoded and decoded images\"\"\"\n",
    "    code = encoder.predict(img[None])[0]\n",
    "    reco = decoder.predict(code[None])[0]\n",
    "\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(\"Original\")\n",
    "    plt.imshow(img)\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.title(\"Code\")\n",
    "    plt.imshow(code.reshape([code.shape[-1]//2,-1]))\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title(\"Reconstructed\")\n",
    "    plt.imshow(reco.clip(0,1))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = autoencoder.evaluate(X_test,X_test,verbose=0)\n",
    "print(\"Final MSE:\",score)\n",
    "\n",
    "for i in range(5):\n",
    "    img = X_test[i]\n",
    "    visualize(img,encoder,decoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going deeper\n",
    "\n",
    "PCA is neat but surely we can do better. This time we want you to build a deep autoencoder by... stacking more layers.\n",
    "\n",
    "In particular, your encoder and decoder should be at least 3 layers deep each. You can use any nonlinearity you want and any number of hidden units in non-bottleneck layers provided you can actually afford training it.\n",
    "\n",
    "![layers](https://pbs.twimg.com/media/CYggEo-VAAACg_n.png:small)\n",
    "\n",
    "A few sanity checks:\n",
    "* There shouldn't be any hidden layer smaller than bottleneck (encoder output).\n",
    "* Don't forget to insert nonlinearities between intermediate dense layers.\n",
    "* Convolutional layers are allowed but not required. To undo convolution use L.Deconv2D, pooling - L.UpSampling2D.\n",
    "* Adding activation after bottleneck is allowed, but not strictly necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_deep_autoencoder(img_shape,code_size=32):\n",
    "    \"\"\"PCA's deeper brother. See instructions above\"\"\"\n",
    "    H,W,C = img_shape\n",
    "    \n",
    "    encoder = keras.models.Sequential()\n",
    "    encoder.add(L.InputLayer(img_shape))\n",
    "    \n",
    "    <Your code: define encoder as per instructions above>\n",
    "    \n",
    "    \n",
    "    decoder = keras.models.Sequential()\n",
    "    decoder.add(L.InputLayer((code_size,)))\n",
    "\n",
    "    <Your code: define encoder as per instructions above>\n",
    "    \n",
    "    return encoder,decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check autoencoder shapes along different code_sizes\n",
    "get_dim = lambda layer: np.prod(layer.output_shape[1:])\n",
    "for code_size in [1,8,32,128,512,1024]:\n",
    "    encoder,decoder = build_deep_autoencoder(img_shape,code_size=code_size)\n",
    "    print(\"Testing code size %i\" % code_size)\n",
    "    assert encoder.output_shape[1:]==(code_size,),\"encoder must output a code of required size\"\n",
    "    assert decoder.output_shape[1:]==img_shape,   \"decoder must output an image of valid shape\"\n",
    "    assert len(encoder.trainable_weights)>=6,     \"encoder must contain at least 3 dense layers\"\n",
    "    assert len(decoder.trainable_weights)>=6,     \"decoder must contain at least 3 dense layers\"\n",
    "    \n",
    "    for layer in encoder.layers + decoder.layers:\n",
    "        assert get_dim(layer) >= code_size, \"Encoder layer %s is smaller than bottleneck (%i units)\"%(layer.name,get_dim(layer))\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hint:__ if you're getting \"Encoder layer is smaller than bottleneck\" error, use code_size when defining intermediate layers. \n",
    "\n",
    "For example, such layer may have code_size*2 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder,decoder = build_deep_autoencoder(img_shape,code_size=32)\n",
    "\n",
    "inp = L.Input(img_shape)\n",
    "code = encoder(inp)\n",
    "reconstruction = decoder(code)\n",
    "\n",
    "autoencoder = keras.models.Model(inp,reconstruction)\n",
    "autoencoder.compile('adamax','mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training may take some 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(x=X_train,y=X_train,epochs=32,\n",
    "                validation_data=[X_test,X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reconstruction_mse = autoencoder.evaluate(X_test,X_test,verbose=0)\n",
    "assert reconstruction_mse <= 0.005, \"Compression is too lossy. See tips below.\"\n",
    "assert len(encoder.output_shape)==2 and encoder.output_shape[1]==32, \"Make sure encoder has code_size units\"\n",
    "print(\"Final MSE:\", reconstruction_mse)\n",
    "for i in range(5):\n",
    "    img = X_test[i]\n",
    "    visualize(img,encoder,decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tips:__ If you keep getting \"Compression to lossy\" error, there's a few things you might try:\n",
    "\n",
    "* Make sure it converged. Some architectures need way more than 32 epochs to converge. They may fluctuate a lot, but eventually they're going to get good enough to pass. You may train your network for as long as you want.\n",
    "\n",
    "* Complexity. If you already have, like, 152 layers and still not passing threshold, you may wish to start from something simpler instead and go in small incremental steps.\n",
    "\n",
    "* Architecture. You can use any combination of layers (including convolutions, normalization, etc) as long as __encoder output only stores 32 numbers per training object__. \n",
    "\n",
    "A cunning learner can circumvent this last limitation by using some manual encoding strategy, but he is strongly recommended to avoid that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising AutoEncoder\n",
    "\n",
    "Let's now make our model into a denoising autoencoder.\n",
    "\n",
    "We'll keep your model architecture, but change the way it trains. In particular, we'll corrupt it's input data randomly before each epoch.\n",
    "\n",
    "There are many strategies to apply noise. We'll implement two popular one: adding gaussian noise and using dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_gaussian_noise(X,sigma=0.1):\n",
    "    \"\"\"\n",
    "    adds noise from normal distribution with standard deviation sigma\n",
    "    :param X: image tensor of shape [batch,height,width,3]\n",
    "    \"\"\"\n",
    "    \n",
    "    <your code here>\n",
    "        \n",
    "    return X + noise\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#noise tests\n",
    "theoretical_std = (X[:100].std()**2 + 0.5**2)**.5\n",
    "our_std = apply_gaussian_noise(X[:100],sigma=0.5).std()\n",
    "assert abs(theoretical_std - our_std) < 0.01, \"Standard deviation does not match it's required value. Make sure you use sigma as std.\"\n",
    "assert abs(apply_gaussian_noise(X[:100],sigma=0.5).mean() - X[:100].mean()) < 0.01, \"Mean has changed. Please add zero-mean noise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(1,4,1)\n",
    "plt.imshow(X[0])\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(apply_gaussian_noise(X[:1],sigma=0.01)[0])\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(apply_gaussian_noise(X[:1],sigma=0.1)[0])\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(apply_gaussian_noise(X[:1],sigma=0.5)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder,decoder = build_deep_autoencoder(img_shape,code_size=512)\n",
    "assert encoder.output_shape[1:]==(512,), \"encoder must output a code of required size\"\n",
    "\n",
    "inp = L.Input(img_shape)\n",
    "code = encoder(inp)\n",
    "reconstruction = decoder(code)\n",
    "\n",
    "autoencoder = keras.models.Model(inp,reconstruction)\n",
    "autoencoder.compile('adamax','mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    print(\"Epoch %i/50, Generating corrupted samples...\"%i)\n",
    "    X_train_noise = apply_gaussian_noise(X_train)\n",
    "    X_test_noise = apply_gaussian_noise(X_test)\n",
    "    \n",
    "    autoencoder.fit(x=X_train_noise,y=X_train,epochs=1,\n",
    "                    validation_data=[X_test_noise,X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ if it hasn't yet converged, increase the number of iterations.\n",
    "\n",
    "__Bonus:__ replace gaussian noise with masking random rectangles on image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "denoising_mse = autoencoder.evaluate(apply_gaussian_noise(X_test),X_test,verbose=0)\n",
    "print(\"Final MSE:\", denoising_mse)\n",
    "for i in range(5):\n",
    "    img = X_test[i]\n",
    "    visualize(img,encoder,decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder.save(\"./encoder.h5\")\n",
    "decoder.save(\"./decoder.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image retrieval with autoencoders\n",
    "\n",
    "So we've just trained a network that converts image into itself imperfectly. This task is not that useful in and of itself, but it has a number of awesome side-effects. Let's see it in action.\n",
    "\n",
    "First thing we can do is image retrieval aka image search. We we give it an image and find similar images in latent space. \n",
    "\n",
    "To speed up retrieval process, we shall use Locality-Sensitive Hashing on top of encoded vectors. We'll use scikit-learn's implementation for simplicity. In practical scenario, you may want to use [specialized libraries](https://erikbern.com/2015/07/04/benchmark-of-approximate-nearest-neighbor-libraries.html) for better performance and customization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = X_train\n",
    "codes = <encode all images>\n",
    "assert len(codes) == len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LSHForest\n",
    "lshf = LSHForest(n_estimators=50).fit(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_similar(image, n_neighbors=5):\n",
    "    assert image.ndim==3,\"image must be [batch,height,width,3]\"\n",
    "\n",
    "    code = encoder.predict(image[None])\n",
    "    \n",
    "    (distances,),(idx,) = lshf.kneighbors(code,n_neighbors=n_neighbors)\n",
    "    \n",
    "    return distances,images[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_similar(image):\n",
    "    \n",
    "    distances,neighbors = get_similar(image,n_neighbors=11)\n",
    "    \n",
    "    plt.figure(figsize=[8,6])\n",
    "    plt.subplot(3,4,1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Original image\")\n",
    "    \n",
    "    for i in range(11):\n",
    "        plt.subplot(3,4,i+2)\n",
    "        plt.imshow(neighbors[i])\n",
    "        plt.title(\"Dist=%.3f\"%distances[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#smiles\n",
    "show_similar(X_test[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ethnicity\n",
    "show_similar(X_test[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#glasses\n",
    "show_similar(X_test[66])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bonus: cheap image morphing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for _ in range(5):\n",
    "    image1,image2 = X_test[np.random.randint(0,len(X_test),size=2)]\n",
    "\n",
    "    code1, code2 = encoder.predict(np.stack([image1,image2]))\n",
    "\n",
    "    plt.figure(figsize=[10,4])\n",
    "    for i,a in enumerate(np.linspace(0,1,num=7)):\n",
    "\n",
    "        output_code = code1*(1-a) + code2*(a)\n",
    "        output_image = decoder.predict(output_code[None])[0]\n",
    "\n",
    "        plt.subplot(1,7,i+1)\n",
    "        plt.imshow(output_image)\n",
    "        plt.title(\"a=%.2f\"%a)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Of course there's a lot more you can do with autoencoders.\n",
    "\n",
    "If you want to generate images from scratch, however, we recommend you our honor track seminar about generative adversarial networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
