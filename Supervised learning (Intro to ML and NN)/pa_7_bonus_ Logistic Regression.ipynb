{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent For Logistic Regression\n",
    "## Handwritten digit recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_digit(x):\n",
    "    plt.axis('off')\n",
    "    plt.imshow(x.reshape((28,28)), cmap=plt.cm.gray)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16e64a7c5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "y = mnist.target\n",
    "X = mnist.data.astype('float64')\n",
    "np.random.shuffle(X)\n",
    "show_digit(X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  80.\n",
      "  175. 176. 219.  16.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   4.  82. 244.\n",
      "  254. 213. 144.  26.   4.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 150. 254. 254.\n",
      "  254. 254. 254. 254. 195. 155.  45.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  15. 218. 254. 247.\n",
      "  234. 153. 134. 140. 240. 254. 244. 115.   6.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 128. 254. 158.  50.\n",
      "    0.   0.   0.   0.  22. 128. 233. 254.  94.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.  81. 252. 244.  62.   0.\n",
      "    0.   0.   0.   0.   0.   0.  90. 254. 210.   5.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   7. 202. 254.  97.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.  22. 241. 254.  10.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.  36. 254. 238.  32.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0. 156. 254.  10.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0. 158. 254. 111.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0. 235. 185.   3.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.  65. 254. 240.  20.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.  80. 247.  93.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0. 107. 254. 177.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.  13. 192. 217.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0. 165. 250.  61.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0. 149. 254.  96.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   9. 241. 234.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.  32. 239. 181.   3.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.  11. 254. 234.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   5. 160. 210.  25.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.  50. 254. 234.   0.   0.   0.   0.   0.\n",
      "    0.   0.   6. 118. 254. 120.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.  11. 254. 234.   0.   0.   0.   0.   0.\n",
      "    0.   4. 198. 254. 173.   9.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   1. 172. 253. 137.   4.   0.   0.   0.\n",
      "   41. 216. 254. 173.   7.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.  91. 254. 254. 184.  91.  41. 107.\n",
      "  228. 253. 140.  10.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   3. 143. 240. 254. 254. 250. 252.\n",
      "  246. 166.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.  67. 154. 227. 254. 217.\n",
      "   85.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "print (X[1].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT since the 0th column of X is 1's we will NOT scale the 0th column. (Hint: set mu_vec[0] to 0 and \n",
    "# sigma_vec[0] to 1) \n",
    "def feature_scale_train(X):\n",
    "    mu_vec = np.average(X[:,1:], axis=0)\n",
    "    mu_vec = np.hstack(([0],mu_vec))\n",
    "    sigma_vec = np.std(X[:, 1:], axis=0)\n",
    "    sigma_vec = np.hstack(([1],sigma_vec))\n",
    "    sigma_vec[sigma_vec==0]=1\n",
    "    X_scaled = (X-mu_vec)/sigma_vec\n",
    "    return X_scaled, mu_vec, sigma_vec\n",
    "\n",
    "def feature_scale_test(X, mu_vec, sigma_vec):\n",
    "    return (X-mu_vec)/sigma_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_scale(beta, mu, sigma):\n",
    "    beta_s = beta * sigma\n",
    "    beta_s[0] = np.sum(beta_s[1:]*mu[1:]/sigma[1:]) + beta[0]\n",
    "    return beta_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_unscale(beta_s, mu, sigma):\n",
    "    beta = beta_s /sigma\n",
    "    beta[0] = beta_s[0]- np.sum(beta_s[1:]*mu[1:]/sigma[1:])\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    e_z = np.exp(z - np.max(z))\n",
    "    return e_z / e_z.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computes the AVERAGE cost function\n",
    "def cost_function(beta, X, y,lam,cnt=None):\n",
    "    #cost2 is the regularization cost\n",
    "    cost2 = lam*norm(beta, ord = 2)\n",
    "    #cost1 is the main cost\n",
    "    h = sigmoid(beta@X.T)\n",
    "    cost1 = -sum(y*np.log(h))\n",
    "    return (cost1+cost2)/(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(data, targets, w):\n",
    "    true_predict = 0\n",
    "    #y = sigmoid(w@data.T)\n",
    "    for i in range(len(targets)):\n",
    "        y = sigmoid(w@data[i])\n",
    "        print(y, targets[i])\n",
    "        if sum(abs(y - targets[i]))/2 <= 0.5:\n",
    "            true_predict += 1\n",
    "    return true_predict/len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.98687660e-01 3.22954647e-02 3.75566675e-09]\n",
      "0.00010102919390777272 [1 0 0]\n",
      "0.8320183851339241 [0 1 0]\n",
      "1.869836380077212e-10 [0 0 1]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Пример\n",
    "beta = np.array([1, 0.3, -2.7])\n",
    "X = np.array([[1,2, 4], [7,9, 3], [1, 3, 9]])\n",
    "target = np.array([[1, 0, 0], [0, 1, 0], [0,0,1]])\n",
    "sigm = sigmoid(beta@X)\n",
    "soft = softmax(beta@X)\n",
    "print(sigm)\n",
    "#print('sigmoid for h is:', sigm, sum(sigm),'\\n', 'softmax for h is:', soft, sum(soft))\n",
    "print(score(X, target, beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computes the gradient for LR\n",
    "# Note, there was a slight typo in the lecture notes \n",
    "# See the new notes posted in slack\n",
    "def compute_gradient(X,y,current_beta, lam):\n",
    "    ...\n",
    "    grad[1:] += lam*current_beta[1:]\n",
    "    grad/=X.shape[0]\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Costs is the dictionary for storing cost_function results after each epoch\n",
    "# test_costs is the dict for storing cost_function results of the TEST DATA after each epoch\n",
    "# errors is the dict for storing error values in percentage (100% - accuracy) after each epoch\n",
    "def logistic_regression_GD(X_unscaled, y, lam, alpha, max_cost_before_break=1000, time_limit=1000, max_count=None,X_test_unscaled=None,y_test=None):\n",
    "    costs = {}\n",
    "    times = {}\n",
    "    test_costs = {}\n",
    "    errors = {}\n",
    "    X, mu, sigma = feature_scale_train(X_unscaled)\n",
    "    X_test = feature_scale_test(X_test_unscaled, mu, sigma)\n",
    "    beta = np.zeros(X.shape[1])\n",
    "    cnt = 0\n",
    "    prev_cost = float(\"inf\")\n",
    "    begin = time.time()\n",
    "    while True:\n",
    "        #while np.abs(prev_cost - curr_cost) >= eps and cnt <= max_epoch :\n",
    "        test_costs[cnt] = cost_function(beta, X_test, y_test, lam)\n",
    "        print('y_test[0] is:', y_test[0])\n",
    "        errors[cnt] = 100 - 100*score(X_test, y_test, beta)\n",
    "        end = time.time()\n",
    "        if int(end-begin) not in times:\n",
    "            times[int(end-begin)] = cnt\n",
    "            print (\"%s seconds has passed at count %s out of %s\" % (end-begin, cnt, max_count))\n",
    "        if curr_cost > max_cost_before_break:\n",
    "            break\n",
    "        if cnt > max_count:\n",
    "            break\n",
    "        prev_cost = curr_cost\n",
    "        h = sigmoid(beta@X)\n",
    "        beta -= alpha*(y - h)@X  + lam*2*beta \n",
    "        curr_cost = cost_function(X, y, beta, lam)\n",
    "        costs[cnt] = curr_cost\n",
    "        cnt += 1\n",
    "    return beta_unscale(beta, mu, sigma), costs, test_costs, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_vs_one_classification(total_proportion, test_proportion, label_one, label_two,alpha_list,max_count):\n",
    "    from sklearn.datasets import fetch_mldata\n",
    "    mnist = fetch_mldata('MNIST original')\n",
    "    y = mnist.target\n",
    "    X = mnist.data.astype('float64')\n",
    "    \n",
    "    # Your code here\n",
    "    # Filter the data and bring it to a shape which is suitable for the gradient descent\n",
    "    #\n",
    "    #\n",
    "    ####### \n",
    "    \n",
    "    X = np.column_stack([np.ones(X.shape[0]),X])\n",
    "    \n",
    "    \n",
    "    #Make sure you understand what is happening here\n",
    "    #np.random.seed(0)\n",
    "    ind = np.random.permutation(X.shape[0])\n",
    "    total_count = int(X.shape[0]*total_proportion)\n",
    "    train_count = int(total_count-test_proportion*total_count)\n",
    "\n",
    "    X_train = X[ind[:train_count]]\n",
    "    y_train = y[ind[:train_count]]\n",
    "    X_test = X[ind[train_count:total_count]]\n",
    "    y_test = y[ind[train_count:total_count]]\n",
    "   \n",
    "\n",
    "    ## Print out their dimensions\n",
    "    print(\"Training dataset dimensions: \", np.shape(X_train))\n",
    "    print(\"Number of training labels: \", len(y_train))\n",
    "    print(\"Testing dataset dimensions: \", np.shape(X_test))\n",
    "    print(\"Number of testing labels: \", len(y_test))\n",
    "    print (\"\\n\")\n",
    "    print(y_train[y_train==1].shape[0])\n",
    "    print(y_train[y_train==0].shape[0])\n",
    "    \n",
    "    lam = 0\n",
    "    \n",
    "    plt.figure(1,figsize=(20,8))\n",
    "    plt.figure(2,figsize=(20,8))\n",
    "    plt.figure(3,figsize=(20,8))\n",
    "    \n",
    "    for alpha in alpha_list:\n",
    "        begin = time.time()\n",
    "        print (\"Starting GD for alpha=%s\" % alpha)\n",
    "        beta_grad, costs, test_costs,errors = logistic_regression_GD(X_train,y_train,lam,alpha,max_count=max_count, X_test_unscaled=X_test,y_test=y_test)\n",
    "        end = time.time()\n",
    "        print (\"Execution time =%s\" % (end-begin))\n",
    "        print (\"Beta_grad COST is=%s\" % cost_function(beta_grad,X_train,y_train,lam))\n",
    "        y_predicted = sigmoid(X_test.dot(beta_grad))\n",
    "        y_predicted[y_predicted>=0.5]=1\n",
    "        y_predicted[y_predicted<0.5]=0\n",
    "        error = np.average((y_predicted-y_test)**2)\n",
    "        print (\"error = %s percent\" % (100*error))\n",
    "        print(\"\\n\")\n",
    "        plt.figure(1)\n",
    "        plt.subplot(131)\n",
    "        plt.loglog(costs.keys(),costs.values(), label=\"alpha=%s and best_cost=%.4lf\" % (alpha, min(costs.values())))\n",
    "        plt.subplot(132)\n",
    "        plt.loglog(test_costs.keys(),test_costs.values(), label=\"alpha=%s and best_cost=%.4lf\" % (alpha, min(costs.values())))\n",
    "        plt.subplot(133)\n",
    "        plt.ylim(0,4)\n",
    "        plt.loglog(errors.keys(),errors.values(), label=\"alpha=%s and best_cost=%.4lf\" % (alpha, min(costs.values())))\n",
    "        \n",
    "        \n",
    "        plt.figure(2)\n",
    "        plt.subplot(131)\n",
    "        plt.semilogy(costs.keys(),costs.values(), label=\"alpha=%s and best_cost=%.4lf\" % (alpha, min(costs.values())))\n",
    "        plt.subplot(132)\n",
    "        plt.semilogy(test_costs.keys(),test_costs.values(), label=\"alpha=%s and best_cost=%.4lf\" % (alpha, min(costs.values())))\n",
    "        plt.subplot(133)\n",
    "        plt.ylim(0,4)\n",
    "        plt.semilogy(errors.keys(),errors.values(), label=\"alpha=%s and best_cost=%.4lf\" % (alpha, min(costs.values())))\n",
    "        \n",
    "        \n",
    "        plt.figure(3)\n",
    "        plt.subplot(131)\n",
    "        plt.plot(costs.keys(),costs.values(), label=\"alpha=%s and best_cost=%.4lf\" % (alpha, min(costs.values())))\n",
    "        plt.subplot(132)\n",
    "        plt.plot(test_costs.keys(),test_costs.values(), label=\"alpha=%s and best_cost=%.4lf\" % (alpha, min(costs.values())))\n",
    "        plt.subplot(133)\n",
    "        plt.ylim(0,4)\n",
    "        plt.plot(errors.keys(),errors.values(), label=\"alpha=%s and best_cost=%.4lf\" % (alpha, min(costs.values())))\n",
    "        \n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.legend()\n",
    "    plt.title(\"Learning curve,loglog\")\n",
    "    plt.xlabel(\"Number of steps\")\n",
    "    plt.ylabel(\"Cost Function\")\n",
    "\n",
    "    plt.figure(2)\n",
    "    plt.legend()\n",
    "    plt.title(\"Learning curve, SemilogY\")\n",
    "    plt.xlabel(\"Number of steps\")\n",
    "    plt.ylabel(\"Cost Function\")\n",
    "\n",
    "    plt.figure(3)\n",
    "    plt.title(\"Learning curve, Real\")\n",
    "    plt.xlabel(\"Number of steps\")\n",
    "    plt.ylabel(\"Cost Function\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After running the following code, results similar to below should appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset dimensions:  (14700, 785)\n",
      "Number of training labels:  14700\n",
      "Testing dataset dimensions:  (6300, 785)\n",
      "Number of testing labels:  6300\n",
      "\n",
      "\n",
      "1657\n",
      "1443\n",
      "Starting GD for alpha=0.01\n",
      "y_test[0] is: 5.0\n",
      "0.5 5.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-9ed7e82602ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmax_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3000\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0malpha_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mone_vs_one_classification\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_proportion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_proportion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_one\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel_two\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-109-b108342625e2>\u001b[0m in \u001b[0;36mone_vs_one_classification\u001b[1;34m(total_proportion, test_proportion, label_one, label_two, alpha_list, max_count)\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mbegin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Starting GD for alpha=%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[0mbeta_grad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcosts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_costs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogistic_regression_GD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlam\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_unscaled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Execution time =%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-108-3439798a106b>\u001b[0m in \u001b[0;36mlogistic_regression_GD\u001b[1;34m(X_unscaled, y, lam, alpha, max_cost_before_break, time_limit, max_count, X_test_unscaled, y_test)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mtest_costs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcnt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'y_test[0] is:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0merrors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcnt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtimes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-92-020cdf9a7cb7>\u001b[0m in \u001b[0;36mscore\u001b[1;34m(data, targets, w)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0.5\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0mtrue_predict\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrue_predict\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16e0e7825c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16e0e7c85c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16e0e7c80b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_proportion = 0.3\n",
    "test_proportion = 0.3\n",
    "label_one = 2\n",
    "label_two = 7\n",
    "max_count = 3000\n",
    "alpha_list = [0.01,0.1,2,10000]\n",
    "one_vs_one_classification(total_proportion,test_proportion, label_one,label_two, alpha_list,max_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_vs_all():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
