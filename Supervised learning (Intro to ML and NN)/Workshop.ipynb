{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = [2, 3, 3, 1, 5, 2]\n",
    "#a = [2, 4, 3, 5, 1]\n",
    "a = [2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Вывод первого повторяющегося элемента , линейно и используя константную память\n",
    "def firstDuplicate(a):\n",
    "    lenght = len(a)\n",
    "    k = 0\n",
    "    for i in range(lenght):\n",
    "        if a[i] > lenght:\n",
    "            k = (a[i] -1)%lenght\n",
    "            if a[k] > lenght:\n",
    "                if (a[i])%lenght == 0:\n",
    "                    return lenght\n",
    "                return (a[i])%lenght\n",
    "        else:\n",
    "            k = a[i]-1\n",
    "            if a[k] > lenght:\n",
    "                return a[i]\n",
    "        a[k] += lenght\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstDuplicate(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person :\n",
    "    def __init__(self, birthday, surname, name):\n",
    "        self.birthday = birthday\n",
    "        self.surname = surname\n",
    "        self.name = name\n",
    "    def __str__(self):\n",
    "        return str(self.birthday) + ', ' +self.surname + ', ' + self.name\n",
    "    def __repr__(self):\n",
    "        return f\"People :{'(' + str(self.birthday) + ', ' +self.surname + ', ' + self.name + ')'}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[People :([1994, 5, 10], Pupkin, Vasya), People :([1995, 3, 9], Apresyan, Petya), People :([1995, 3, 9], Xabenskiy, Kostya)]\n"
     ]
    }
   ],
   "source": [
    "def byAge_surname_name(person):\n",
    "    return person.birthday, person.surname, person.name\n",
    "\n",
    "p1=Person([1994, 5, 10], \"Pupkin\", \"Vasya\")\n",
    "p2=Person([1995, 3, 9], \"Xabenskiy\", \"Kostya\")\n",
    "p3=Person([1995, 3, 9], \"Apresyan\", \"Petya\")\n",
    "people = [p1, p2, p3]\n",
    "people = sorted(people, key = byAge_surname_name)\n",
    "print(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vector3D:\n",
    "    def __init__(self, Vx, Vy, Vz):\n",
    "        self.Vx = Vx\n",
    "        self.Vy = Vy\n",
    "        self.Vz = Vz\n",
    "    def __str__(self):\n",
    "        return '(' + str(self.Vx) + ', ' + str(self.Vy) + ', ' + str(self.Vz) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr.sort(key = attrgetter('to_string'))\n",
    "# print(arr[1].surname)\n",
    "# p1.__init__([1994, 2, 7],\"Vasya\", \"Pupkin\")\n",
    "# p2.__init__([1995, 3, 9],\"Petya\", \"Shishkin\")\n",
    "# p3.__init__([1995, 3, 9],\"Kostya\", \"Xabenskiy\")\n",
    "# self.to_string = str(birthday[0])+str(birthday[1])+str(birthday[2])+str(surname)+str(name)\n",
    "# print (self.name, self.surname, self.birthday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import numpy as np\n",
    "# import matplotlib\n",
    "# from matplotlib import pyplot as plt\n",
    "# import scipy\n",
    "# from sklearn import svm\n",
    "\n",
    "\n",
    "# mu_vec1 = np.array([0,0])\n",
    "# cov_mat1 = np.array([[2,0],[0,2]])\n",
    "# x1_samples = np.random.multivariate_normal(mu_vec1, cov_mat1, 100)\n",
    "# mu_vec1 = mu_vec1.reshape(1,2).T # to 1-col vector\n",
    "\n",
    "# mu_vec2 = np.array([1,2])\n",
    "# cov_mat2 = np.array([[1,0],[0,1]])\n",
    "# x2_samples = np.random.multivariate_normal(mu_vec2, cov_mat2, 100)\n",
    "# mu_vec2 = mu_vec2.reshape(1,2).T\n",
    "\n",
    "\n",
    "# fig = plt.figure()\n",
    "\n",
    "\n",
    "# plt.scatter(x1_samples[:,0],x1_samples[:,1], marker='+')\n",
    "# plt.scatter(x2_samples[:,0],x2_samples[:,1], c= 'green', marker='o')\n",
    "\n",
    "# X = np.concatenate((x1_samples,x2_samples), axis = 0)\n",
    "# Y = np.array([0]*100 + [1]*100)\n",
    "\n",
    "# C = 1.0  # SVM regularization parameter\n",
    "# clf = svm.SVC(kernel = 'linear',  gamma=0.7, C=C )\n",
    "# clf.fit(X, Y)\n",
    "\n",
    "# w = clf.coef_[0]\n",
    "# a = -w[0] / w[1]\n",
    "# xx = np.linspace(-5, 5)\n",
    "# yy = a * xx - (clf.intercept_[0]) / w[1]\n",
    "\n",
    "# plt.plot(xx, yy, 'k-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = model.coef_[0]\n",
    "# const = model.intercept_[0]\n",
    "# a = -w[1] / w[2]\n",
    "# xx = np.linspace(-1.5, 3.5)\n",
    "\n",
    "# yy = a * xx - (const) / w[2]\n",
    "# y1 = a* xx - (const + 1) / w[2]\n",
    "# y2 = a* xx - (const - 1) / w[2]\n",
    "# plt.plot(xx, y1, 'r--')\n",
    "# plt.plot(xx, y2, 'r--')\n",
    "# plt.plot(xx, yy, 'r')\n",
    "# Plot_2d_graphic(X_1, y_1)\n",
    "# sv = model.support_vectors_\n",
    "# s = model.support_\n",
    "# dc = model.dual_coef_\n",
    "# print(dc,model.coef_, sv)\n",
    "\n",
    "# #contour()\n",
    "# h = 0.2\n",
    "# x_min, x_max = X_1[:,0].min() - 1, X_1[:, 0].max() + 1\n",
    "# y_min, y_max = X_1[:,1].min() - 1, X_1[:, 1].max() + 1\n",
    "# xx, yy = np.meshgrid(\n",
    "#     np.arange(x_min, x_max, h),\n",
    "#     np.arange(y_min, y_max, h))\n",
    "\n",
    "# # create decision boundary plot\n",
    "# Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "# Z = Z.reshape(xx.shape)\n",
    "# contour(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "# plt.scatter(X_1[:,0],X_1[:,1])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class A(object):\n",
    "    \n",
    "#     def __init__(self, l=0.1, h = 5000):\n",
    "#         self.w_1 = np.random.normal(-0.1, 0.1, (50, 64))\n",
    "#         self.w_2 = np.random.normal(-0.1, 0.1, (10, 50))\n",
    "        \n",
    "#     def a(self):\n",
    "#         print(self.w_1)\n",
    "#         self.w_1 = 0\n",
    "#         print(self.w_1)\n",
    "        \n",
    "# a = A(1,2)\n",
    "# a.w_1 += 500\n",
    "# print(a.a())\n",
    "\n",
    "\n",
    "# def th(z):\n",
    "#     return (np.exp(2*z) - 1)/(np.exp(2*z) + 1)\n",
    "\n",
    "# def softmax(z):\n",
    "#     e_z = np.exp(z - np.max(z))\n",
    "#     return e_z / e_z.sum()\n",
    "\n",
    "# w_1 = np.random.normal(-0.1, 0.1, (50, 64))\n",
    "# w_2 = np.random.normal(-0.1, 0.1, (10, 50))\n",
    "# b_1 = np.random.normal(-0.1, 0.1, (50, 1))\n",
    "# b_2 = np.random.normal(-0.1, 0.1, (10, 1))\n",
    "\n",
    "# z_1 = w_1@x.T + b_1\n",
    "# h = np.tanh(z_1)\n",
    "# z_2 = w_2@h + b_2\n",
    "# y = softmax(z_2)\n",
    "\n",
    "# x_0 = X_train[0].reshape(1, 64)\n",
    "# d_w1 = ((y - y_train[0]).T@w_2).T*(1-h**2)@x_0\n",
    "# d_w2 = (y - y_train[0])*h.T\n",
    "# #print(sum(d_w1), sum((1- h**2)@x_0))\n",
    "\n",
    "# # d_w1 = ((y - y_train[i]).T@w_2).T*(1-h**2)@x_0\n",
    "# # d_w2 = (y-t[i])*h.T\n",
    "# #x_0 = X_train[0].reshape(1, 64)\n",
    "# #                 print(z_1.shape, y.shape)\n",
    "# #                 print(((y-t[i]).T@w_2).T.shape, ((1-h**2).shape), x_i.shape)\n",
    "# # t = np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "# # t = t.reshape(10,1)\n",
    "# # x = np.random.normal(-0.1, 0.1, (64, 1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD (model, X_train, y_train, minibatch_size):\n",
    "    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n",
    "\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        idx = np.random.randint(0, len(minibatches))\n",
    "        X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "        grad = get_minibatch_grad(model, X_mini, y_mini)\n",
    "\n",
    "        for layer in grad:\n",
    "            model[layer] += alpha * grad[layer]\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def momentum(model, X_train, y_train, minibatch_size):\n",
    "    velocity = {k: np.zeros_like(v) for k, v in model.items()}\n",
    "    gamma = .9\n",
    "\n",
    "    minibatches = get_minibatch(X_train, y_train, minibatch_size)\n",
    "\n",
    "    for iter in range(1, n_iter + 1):\n",
    "        idx = np.random.randint(0, len(minibatches))\n",
    "        X_mini, y_mini = minibatches[idx]\n",
    "\n",
    "        grad = get_minibatch_grad(model, X_mini, y_mini)\n",
    "\n",
    "        for layer in grad:\n",
    "            velocity[layer] = gamma * velocity[layer] + alpha * grad[layer]\n",
    "            model[layer] += velocity[layer]\n",
    "\n",
    "    return model\n",
    "\n",
    "# the schematic structure of the code for mini-batch gradient descent looks as follows\n",
    "n_batch = 4; # size of the batch, e.g. 4,8\n",
    "n_epoch = 50;\n",
    "\n",
    "def iterate_minibatches(X, y, n_batch):\n",
    "    indices = np.arange(len(y))\n",
    "    np.random.shuffle(indices)\n",
    "    batches = []\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        batch_indices = indices[start_idx:start_idx + batchsize]\n",
    "        X_i = [X[i] for i in batch_indices]\n",
    "        y_i = [y[i] for i in batch_indices]\n",
    "        batches.append(X_i, y_i)\n",
    "    return batches\n",
    "\n",
    "def fit_logistic_regression():\n",
    "    for i_epoch in range(0,n_epoch):\n",
    "    # shuffle the indices used for minibatch sampling\n",
    "        indices = np.arange(len(X_train))\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "        loss_train = 0\n",
    "        acc_train = 0\n",
    "        \n",
    "        ... = iterate_minibatches(X, y, n_batch):\n",
    "        \n",
    "        for # i_batch in range(0,n_batches):\n",
    "            indices\n",
    "            \n",
    "            X_minibatch = X_train[<todo indices>:,:]\n",
    "            y_batch = y_train[<todo indices>:,:]\n",
    "            \n",
    "            # compute the average gradient for the minibatch\n",
    "            g, loss = compute_gradient(X_minibatch, ... )\n",
    "            \n",
    "            # update the gradient with/without momentum\n",
    "            v = v + ...\n",
    "            dw = g + ...\n",
    "            \n",
    "            W = W - learning_rate * g;\n",
    "            \n",
    "            acc_train += acc_val\n",
    "            loss_train += loss_val\n",
    "        \n",
    "        # plot the loss function for the train set  \n",
    "        loss_train[i_epoch] = loss_total / len(rng)\n",
    "        \n",
    "        # compute the test set accuracy\n",
    "        acc_test = ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
